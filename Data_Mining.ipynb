{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Data Mining.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1uhLQyhmgN8QPlVnTDyJtgPP2Ajuq2v4W",
      "authorship_tag": "ABX9TyMqKeDcj4rMkVeFEqL2+fn7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/karad1818/Python_Project/blob/main/Data_Mining.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fMjDT03ttTao"
      },
      "source": [
        "# Source  : Medium Articles by tarun gupta\r\n",
        "\r\n",
        "# Data : collection of an object and attributes/features\r\n",
        "\r\n",
        "# Characteristics of an attributes :\r\n",
        "# 1. Distinctness : = , !=\r\n",
        "# 2. Order : > , < >= , <=\r\n",
        "# 3. Addition : + , -\r\n",
        "# 4. Multiplication : * , /\r\n",
        "\r\n",
        "# Types of attributes based on characteristics :\r\n",
        "#                       1. Nominal : 1 ex : zip code,ID\r\n",
        "#                       2. Ordinal : 1,2 ex : grades\r\n",
        "#                       3. Interval : 1,2,3 ex : dates\r\n",
        "#                       4. Ratio : 1,2,3,4 ex : height , Weight\r\n",
        "\r\n",
        "# characteristics of data sets :\r\n",
        "# 1. dimensionality : number of attributes that data set have.\r\n",
        "\r\n",
        "# 2. sparsity : For some data sets, such as those with asymmetric features, \r\n",
        "#               most attributes of an object have values of 0\r\n",
        "#               in many cases fewer than 1% of the entries are non-zero. \r\n",
        "#               Such a data is called sparse data or it can be said that the data set has Sparsity.\r\n",
        "\r\n",
        "# 3. resolution : the patterns in the data depend on the level of resolution. \r\n",
        "#                 If the resolution is too fine, a pattern may not be visible or may be buried in noise; \r\n",
        "#                 if the resolution is too coarse, the pattern may disappear.\r\n",
        "\r\n",
        "# the Curse of Dimensionality, it means many types of Data Analysis becomes difficult as the dimensionality  of the data set increases.\r\n",
        "\r\n",
        "# Types of data sets :\r\n",
        "# 1. record data\r\n",
        "# 2. graph-based data\r\n",
        "# 3. ordered-data : ex. genomic sequence data\r\n",
        "\r\n",
        "\r\n",
        "# measurement to check data quality statistically :\r\n",
        "# 1. precision : standard deviation\r\n",
        "# 2. bias \r\n",
        "# 3. accuracy\r\n",
        "\r\n",
        "# example, we have a standard laboratory weight with a mass of 1g and want to assess the precision and bias. \r\n",
        "# We weigh the mass five times, and obtain the following five values: {1.015, 0.990, 1.013, 1 .001, 0.986}.\r\n",
        "# The mean of these values is 1.001, and hence, the bias is 0.001. \r\n",
        "# The precision, as measured by the standard deviation, is 0.013\r\n",
        "\r\n",
        "# Measures of proximity in data-mining :\r\n",
        "\r\n",
        "# generally two proximity measures\r\n",
        "\r\n",
        "# Similarity :\r\n",
        "\r\n",
        "# It is a numerical measure of the degree to which the two objects are alike\r\n",
        "# 0 : no similarity 1 : complete similarity\r\n",
        "# 0 <= similarity <= 1\r\n",
        "\r\n",
        "# Dissimilarity :\r\n",
        "\r\n",
        "# It is a numerical measure of the degree to which the two objects are different.\r\n",
        "# Lower for pair of objects that are more similar\r\n",
        "# 0 <= dissimilarity <= inf\r\n",
        "\r\n",
        "# Transformation function :\r\n",
        "\r\n",
        "# It is a function used to convert similarity to dissimilarity and vice versa\r\n",
        "\r\n",
        "# s' = (s-min(s)) / max(s)-min(s))\r\n",
        "# s' = new proximity measures value\r\n",
        "# s =  old proximity value\r\n",
        "\r\n",
        "# for more info about proximity measures visit : https://tinyurl.com/uzk7rz6\r\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ATxZFfyYEINJ"
      },
      "source": [
        "![](https://miro.medium.com/max/814/1*tKlgcYb4VHRvsCywxCTzlA.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rt-guXLREUOt"
      },
      "source": [
        "# Data preprocessing :\r\n",
        "\r\n",
        "# 1. Aggregation : join two or more into one\r\n",
        "# 2. Sampling : selecting a subset of the data\r\n",
        "#               1. random sampling\r\n",
        "#               2. sampling without replacement\r\n",
        "#               3. sampling with replacement\r\n",
        "#                      1. stratified sampling : split data into several parts and select random from each part\r\n",
        "#                      2. progressive sampling : These approaches start with a small sample, and then increase \r\n",
        "#                                                the sample size until a sample of sufficient size has been obtained.\r\n",
        "# 3. Dimensionality Reduction :\r\n",
        "#                               1. principle component analysis(PCA)\r\n",
        "#                               2. singular value decomposition\r\n",
        "# 4. Feature Subset Selection :\r\n",
        "#                               1. enbedded approach\r\n",
        "#                               2. filter approach\r\n",
        "#                               3. wrapper approach\r\n",
        "# 5. Feature Creation\r\n",
        "# 6. Discretization and Binarization :  Discretization is the process of converting a continuous attribute into an ordinal attribute.\r\n",
        "#                                       Binarization maps a continuous or categorical attribute into one or more binary variables\r\n",
        "# 7. Variable Transformation : pow(x,k) , k*x ....\r\n",
        "#                             1. normalization : subtract mean and divide by standard deviation and ....\r\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jP8CQDGgNUyD"
      },
      "source": [
        "import numpy as np # for numbers\r\n",
        "import pandas as pd # for handling big data set\r\n",
        "from sklearn.impute import SimpleImputer # for handling missing values\r\n",
        "from sklearn.preprocessing import LabelEncoder , OneHotEncoder # use for encoding categorical data\r\n",
        "from sklearn.preprocessing import StandardScaler # used for feature scaling\r\n",
        "from sklearn.model_selection import train_test_split # used for splitting data into test and train\r\n",
        "from sklearn.tree import DecisionTreeClassifier  # decision tree classifier\r\n",
        "from sklearn.metrics import confusion_matrix  #importing confusion  matrix\r\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qu8wJzCNSCGf"
      },
      "source": [
        "df = pd.read_csv(\"/content/drive/MyDrive/Data Mining/Data.csv\")\r\n",
        "# handle missing values :\r\n",
        "# one idea is to erase whole object that contains missing value but it's dangerous sometimes\r\n",
        "# other idea and most comman one is that take the mean and replace it\r\n",
        "\r\n",
        "fill = SimpleImputer(missing_values=np.nan,strategy='mean')\r\n",
        "# Splitting the attributes into independent and dependent attributes\r\n",
        "data = df.values # convert frame into array\r\n",
        "data[:,1:3] = fill.fit_transform(data[:,1:3]) # transform data\r\n",
        "df = data\r\n",
        "# handling categorical data\r\n",
        "# so here we need to encode the text into numbers\r\n",
        "encode = LabelEncoder()\r\n",
        "data[:,0] = encode.fit_transform(data[:,0])\r\n",
        "data[:,3] = encode.fit_transform(data[:,3])\r\n",
        "df = data\r\n",
        "\r\n",
        "# split the data-set into test and train data-set : \r\n",
        "# in most of the algorithm this one is useful\r\n",
        "\r\n",
        "x_train ,x_test , y_train , y_test = train_test_split(data[:,1:2],data[:,2:3],test_size=0.2,random_state=0)\r\n",
        "\r\n",
        "# feature scaling :\r\n",
        "# in this example age is varying from [0,100] and payment [0,100000]\r\n",
        "# so in some model that creates problem because of different scales\r\n",
        "\r\n",
        "scale = StandardScaler()\r\n",
        "x_train = scale.fit_transform(x_train)\r\n",
        "x_test = scale.transform(x_test)  # https://tinyurl.com/y54odzhj\r\n",
        "# same way scale for y values\r\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iT6wsrX12Vjj"
      },
      "source": [
        "# Association Rule Mining :\r\n",
        "\r\n",
        "# Aim : one way to find a pattern\r\n",
        "#       features that occur together , features that are co-related\r\n",
        "\r\n",
        "# ex. if there is rule like milk -> bread that means people who buys milk also buys\r\n",
        "#     but here people who buys bread doesn't mean they buy milk too (watch the directionality)\r\n",
        "\r\n",
        "# When to use association rule mining :\r\n",
        "# there are many example where people do work X and they likely to do work Y and so on..\r\n",
        "\r\n",
        "# term :\r\n",
        "\r\n",
        "# support count (sigma) = frequency of occurance of itemset\r\n",
        "# frequent itemset = itemset whose sigma is >= min_support_threshold\r\n",
        "\r\n",
        "# Measures of effectiveness of the rule :\r\n",
        "\r\n",
        "# ex if rule is {x,y} -> z\r\n",
        "\r\n",
        "# 1. support : sigma({x,y,z})/total\r\n",
        "# 2. confidence : sigma({x,y,z})/sigma({x,y})\r\n",
        "# 3. lift : confidence({x,y}->z) / support(z)\r\n",
        "\r\n",
        "# --> greater lift value means stronger association (near 1 means they often appear as expected)\r\n",
        "\r\n",
        "# for given example :\r\n",
        "\r\n",
        "# association rule = {bread}->{diaper}\r\n",
        "\r\n",
        "# support = sigma({bread,diaper})/5 = 3/5\r\n",
        "# confidence  = sigma({bread,diaper})/sigma({bread}) = 3/4\r\n",
        "# lift = conf({bread,diaper})/support(diaper) = (3/4) / (4/5) = 0.9375\r\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iEIvHvdj8yAs"
      },
      "source": [
        "![](https://miro.medium.com/max/470/1*908489_PRdpPMctC6MT6OQ.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UzUTAt_VCGci"
      },
      "source": [
        "# Apriori Algorithm for Association rule mining :\r\n",
        "\r\n",
        "# IDEA : if an X is a frequent itemset then all the subset of X is also frequent itemset\r\n",
        "#      or we can say that if X is not frequent then superset of X is also not frequent\r\n",
        "\r\n",
        "# Step 1 : find a frequent itemset using above kind of pruning\r\n",
        "# Step 2 : Rule generation using confidence\r\n",
        "\r\n",
        "# if two frequent itemset are ABC, ABD now we can join iff they differ by 1 element and get ABCD\r\n",
        "\r\n",
        "# for second step :\r\n",
        "# frequent itemset = {A,B,C,D}\r\n",
        "# confi(ABC -> D) >= confi(AB -> CD) >= confi(A -> BCD)\r\n",
        "\r\n",
        "# Why ? --> confi(ABC -> D) = supprt(ABCD)/support(ABC)\r\n",
        "#       and for confi(AB -> CD) = support(ABCD)/support(AB)\r\n",
        "#       in here lesser the denominator is higher the confidence.\r\n",
        "#       so support(ABC) <= support(AB) ...\r\n",
        "\r\n",
        "# so for step 2 we can prune same way ..\r\n",
        "\r\n",
        "# But after the algo. we find the many such association rules that satisfy the criteria\r\n",
        "# so now we'll measure interstingness of all rules and select rules that satisfy\r\n",
        "# thera are many measure like lift , interest and so on .."
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RevMnKIVHbIa"
      },
      "source": [
        "![](https://miro.medium.com/max/700/1*_22UGOKJJ6C8B3Zhk_IAiQ.png)\r\n",
        "![](https://i.ibb.co/tZXchJ3/dm.png)\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KPP6OFd2cIkR"
      },
      "source": [
        "!pip install apyori\r\n",
        "from apyori import apriori"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PX70eVdPYgWP"
      },
      "source": [
        "# Implement apriori\r\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/Data Mining/Groceries_dataset.csv\",parse_dates=['Date'])\r\n",
        "# parse_dates is for telling pandas that it's date https://tinyurl.com/y3a5u8cw\r\n",
        "\r\n",
        "# start preprocessing\r\n",
        "\r\n",
        "# check is there any null\r\n",
        "df.isnull().any()\r\n",
        "\r\n",
        "# to know how many unique items are there\r\n",
        "df.itemDescription.unique().shape\r\n",
        "\r\n",
        "# converting item column to row\r\n",
        "other = pd.get_dummies(df.itemDescription)\r\n",
        "other\r\n",
        "\r\n",
        "# now delete item column in original dataset\r\n",
        "df.drop(columns=['itemDescription'],inplace=True)\r\n",
        "df\r\n",
        "\r\n",
        "# join other to df\r\n",
        "df = df.join(other)\r\n",
        "# df.info()\r\n",
        "# df[[\"Member_number\",\"Date\"]].value_counts()\r\n",
        "# end preprocessing"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lsf_8i0tasaQ"
      },
      "source": [
        "\r\n",
        "df = df.groupby([\"Member_number\",\"Date\"]).sum()\r\n",
        "column_name = list(df.columns)\r\n",
        "l = df.values.tolist()\r\n",
        "\r\n",
        "final_list = []\r\n",
        "for i in range(0,len(l),1):\r\n",
        "  temp = []\r\n",
        "  for j in range(0,len(l[i]),1):\r\n",
        "    if l[i][j] > 0:\r\n",
        "      temp.append(column_name[j])\r\n",
        "  final_list.append(temp)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "USj-6ZFyljly"
      },
      "source": [
        "rules = apriori(final_list,min_support=0.00030,min_confidence=0.05,min_lift=3,min_length=2)\r\n",
        "association_rules = list(rules)\r\n",
        "\r\n",
        "for i in association_rules:\r\n",
        "  x = i[0]\r\n",
        "  li = [ p for p in x]\r\n",
        "  print(\"Rules : \" + li[0] + \"  -->  \" + li[1])\r\n",
        "  print(\"support : \" + str(i[1]))\r\n",
        "  print(\"confidence : \" + str(i[2][0][2]))\r\n",
        "  print(\"Lift : \" + str(i[2][0][3]))\r\n",
        "  print(\" -----------------------------------\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nzFgtcZUY451"
      },
      "source": [
        "# Classification :\r\n",
        "\r\n",
        "# IDEA : to classify an item after training \r\n",
        "\r\n",
        "# Decision Tree :\r\n",
        "\r\n",
        "# how to make a best split :\r\n",
        "\r\n",
        "# after every split we get more purity then parents have.\r\n",
        "\r\n",
        "# The decision to split at each node is made according to the metric called purity. \r\n",
        "# A node is 100% impure when a node is split evenly 50/50 and 100% pure when all of its data belongs to a single class.\r\n",
        "\r\n",
        "# In order to optimize our model we need to reach maximum purity and avoid impurity\r\n",
        "\r\n",
        "# The other metric is Information Gain = Entropy(parent) - Weighted Sum of Entropy(Children).\r\n",
        "# some time instead of information gain people uses gini index.\r\n",
        "\r\n",
        "# so at every step find a information gain and select attribute that has maximum information gain repeat untill you get all leaf nodes\r\n",
        "\r\n",
        "# Problem with decision tree is overfitting :\r\n",
        "#        --> may be because of noise\r\n",
        "#        --> may be we are building decision tree based on very few amount of data\r\n",
        "\r\n",
        "# avoid overfitting :\r\n",
        "#  do split until you get 96% or 99% pure class not 100% instead <-- this is known as pruning\r\n",
        "\r\n",
        "# handling continuous values :\r\n",
        "#     choose some threshold(can be more than 1) and convert contiuous value into discrete form\r\n",
        "\r\n",
        "# regression tree : here we use set of attributes instead of class label\r\n",
        "#                   CART (classification and regression tree)\r\n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tSKqCaC_cfyE"
      },
      "source": [
        "# import os\r\n",
        "# os.environ['KAGGLE_CONFIG_DIR'] = \"/content/drive/MyDrive/Data Mining\"\r\n",
        "# %cd /content/drive/MyDrive/Data Mining\r\n",
        "# # put kaggle API\r\n",
        "# !unzip \\*.zip  && rm *.zip"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "orAa64fzj8T4"
      },
      "source": [
        "df = pd.read_csv(\"/content/drive/MyDrive/Data Mining/winequality-red.csv\")\r\n",
        "df.head()\r\n",
        "\r\n",
        "# check and do preprocessing\r\n",
        "df.isnull().any()\r\n",
        "# df.value_counts() 1359\r\n",
        "# df.info() 1599\r\n",
        "\r\n",
        "# there are some redundant rows. because of smaller amount of data if i'll redundant then accuracy will decrease\r\n",
        "# df.drop_duplicates(inplace=True)\r\n",
        "\r\n",
        "# df['quality'].value_counts()\r\n",
        "df['quality']=df['quality'].map(lambda x : 1 if x>6 else 0);\r\n",
        "\r\n",
        "# split data into train and test\r\n",
        "x_train , x_test , y_train , y_test = train_test_split(df.iloc[:,0:11],df.iloc[:,11:],test_size=0.25,random_state=0)\r\n",
        "# x_train.info()\r\n",
        "# x_test.info()"
      ],
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wQX2REJjoPwq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2586eba2-43b9-4e90-d889-777930368ba5"
      },
      "source": [
        "decision_tree = DecisionTreeClassifier()\r\n",
        "decision_tree.fit(x_train,y_train)\r\n",
        "y_pred = decision_tree.predict(x_test)\r\n",
        "y_list = y_test.values.tolist()\r\n",
        "y_ok = []\r\n",
        "for i in range(0,len(y_list),1):\r\n",
        "  y_ok.append(y_list[i][0])\r\n",
        "y_np = np.array(y_ok)\r\n",
        "accuracy_score(y_ok,y_pred)"
      ],
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9025"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w2xk6uTmmHY-"
      },
      "source": [
        "# Bayes Classifier :\r\n",
        "\r\n",
        "# https://tinyurl.com/bayes-classify1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yIu9syGiRALi"
      },
      "source": [
        "![](https://miro.medium.com/max/362/1*6dmvRYysiU5PwWIcHRdKVw.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V-BnbudTQvtn"
      },
      "source": [
        "![](https://i.ibb.co/DK47bQr/Bayes.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gfGDY7JWRPCY"
      },
      "source": [
        "# Advantage :\r\n",
        "# 1. When assumption of independence holds, a Naive Bayes classifier performs better compare to other models like logistic regression and you need less training data.\r\n",
        "# 2. It perform well in case of categorical input variables compared to numerical variable(s). \r\n",
        "#    For numerical variable, normal distribution is assumed (bell curve, which is a strong assumption)\r\n",
        "\r\n",
        "\r\n",
        "# Disadvantage of bayes :\r\n",
        "# 1. If categorical variable has a category (in test data set), which was not observed in training data set, \r\n",
        "#    then model will assign a 0 (zero) probability and will be unable to make a prediction. \r\n",
        "#    This is often known as Zero Frequency. To solve this, we can use the smoothing technique. \r\n",
        "#    One of the simplest smoothing techniques is called Laplace estimation.\r\n",
        "\r\n",
        "# 2. Another limitation of Naive Bayes is the assumption of independent predictors. \r\n",
        "#    In real life, it is almost impossible that we get a set of predictors which are completely independent."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K8NMEtXiTt6V"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}